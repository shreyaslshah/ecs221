{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HY1Ssh7Q4nQz",
        "outputId": "e53f2a7e-94e4-4e59-b13d-4118d6fa6e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shreyasshah/Desktop/Fall_2025/STA 221/ecs221/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[I 2025-11-21 14:26:55,661] A new study created in memory with name: tabtransformer_fixed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using strict deterministic algorithms in PyTorch.\n",
            "Using device: cpu\n",
            "Before oversampling:\n",
            "Churn\n",
            "0    4138\n",
            "1    1495\n",
            "Name: count, dtype: int64\n",
            "Churn\n",
            "0    0.7346\n",
            "1    0.2654\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "After oversampling:\n",
            "0    4138\n",
            "1    4138\n",
            "Name: count, dtype: int64\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:27:44,381] Trial 0 finished with value: 0.8474284062519357 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'dim_feedforward': 128, 'dropout': 0.21242177333881365, 'lr': 0.00010994335574766199, 'weight_decay': 0.0008123245085588687}. Best is trial 0 with value: 0.8474284062519357.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Trial 0] early stopping at epoch 38 (best val AUC=0.8474)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:28:31,203] Trial 1 finished with value: 0.8490492019903785 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'dim_feedforward': 256, 'dropout': 0.04184815819561255, 'lr': 0.0003839629299804173, 'weight_decay': 1.2562773503807034e-05}. Best is trial 1 with value: 0.8490492019903785.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Trial 1] early stopping at epoch 11 (best val AUC=0.8490)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:29:03,068] Trial 2 finished with value: 0.8469741705035823 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 1, 'dim_feedforward': 64, 'dropout': 0.28466566117599995, 'lr': 0.00853618986286683, 'weight_decay': 0.0002661901888489054}. Best is trial 1 with value: 0.8490492019903785.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Trial 2] early stopping at epoch 21 (best val AUC=0.8470)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:29:47,288] Trial 3 finished with value: 0.8467057584704645 and parameters: {'d_model': 128, 'n_heads': 2, 'n_layers': 2, 'dim_feedforward': 128, 'dropout': 0.19875668530619459, 'lr': 0.0004201672054372534, 'weight_decay': 3.632486956676606e-05}. Best is trial 1 with value: 0.8490492019903785.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Trial 3] early stopping at epoch 13 (best val AUC=0.8467)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:31:38,033] Trial 4 finished with value: 0.8499267028678793 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 4, 'dim_feedforward': 128, 'dropout': 0.058794858725743554, 'lr': 0.00012315571723666037, 'weight_decay': 9.462175356461487e-06}. Best is trial 4 with value: 0.8499267028678793.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Trial 4] early stopping at epoch 12 (best val AUC=0.8499)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-21 14:31:43,238] Trial 5 pruned. \n",
            "[I 2025-11-21 14:31:45,107] Trial 6 pruned. \n",
            "[I 2025-11-21 14:32:00,852] Trial 7 pruned. \n",
            "[I 2025-11-21 14:32:05,600] Trial 8 pruned. \n",
            "[I 2025-11-21 14:32:24,545] Trial 9 pruned. \n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Install dependencies\n",
        "# ============================================\n",
        "%pip install optuna --quiet\n",
        "%pip install torch torchvision torchaudio --quiet\n",
        "\n",
        "# ============================================\n",
        "# Imports\n",
        "# ============================================\n",
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ============================================\n",
        "# Global seed & deterministic setup\n",
        "# ============================================\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# cuDNN deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Try strict deterministic algorithms (may raise on some ops/versions)\n",
        "try:\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    print(\"Using strict deterministic algorithms in PyTorch.\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: Could not enable strict deterministic algorithms:\", e)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ============================================\n",
        "# Load dataset and feature selection\n",
        "# ============================================\n",
        "features = [\n",
        "    'RiskScore', 'MonthlyCharges', 'AvgChargesPerMonth', 'ContractRisk', 'Contract',\n",
        "    'TotalServices', 'TotalCharges', 'EstimatedCLV', 'HasFamily', 'tenure',\n",
        "    'Partner', 'HighValueCustomer', 'PaymentRisk', 'Dependents'\n",
        "]\n",
        "\n",
        "df = pd.read_csv('final_telco_engineered.csv')\n",
        "\n",
        "X = df[features]\n",
        "y = df['Churn']  # adjust if your target column name differs\n",
        "\n",
        "# Identify categorical and numerical columns for preprocessing\n",
        "categorical_cols = ['Contract', 'HasFamily', 'Partner', 'HighValueCustomer', 'Dependents']\n",
        "numerical_cols = [col for col in features if col not in categorical_cols]\n",
        "\n",
        "# ============================================\n",
        "# Preprocessing pipeline\n",
        "# ============================================\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_cols),\n",
        "    ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
        "])\n",
        "\n",
        "# ============================================\n",
        "# Train/validation/test split (80/10/10)\n",
        "# ============================================\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.10, random_state=SEED, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=SEED, stratify=y_temp\n",
        ")  # 0.1111 * 0.90 â‰ˆ 0.10\n",
        "\n",
        "# Ensure target variable numeric (0/1)\n",
        "if y_train.dtype == 'O' or y_train.dtype.name == 'category':\n",
        "    mapping = {'No': 0, 'Yes': 1}\n",
        "    y_train = y_train.map(mapping).astype(int)\n",
        "    y_val   = y_val.map(mapping).astype(int)\n",
        "    y_test  = y_test.map(mapping).astype(int)\n",
        "\n",
        "# ============================================\n",
        "# Fit preprocessor on ORIGINAL training data\n",
        "# ============================================\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_val_proc   = preprocessor.transform(X_val)\n",
        "X_test_proc  = preprocessor.transform(X_test)\n",
        "\n",
        "# ============================================\n",
        "# Random oversampling to balance classes (50/50)\n",
        "# ============================================\n",
        "print(\"Before oversampling:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(pd.Series(y_train).value_counts(normalize=True))\n",
        "\n",
        "_df_train = pd.DataFrame(X_train_proc)\n",
        "_df_train['Churn'] = y_train.reset_index(drop=True)\n",
        "\n",
        "_df_majority = _df_train[_df_train['Churn'] == 0]\n",
        "_df_minority = _df_train[_df_train['Churn'] == 1]\n",
        "\n",
        "_df_minority_up = resample(\n",
        "    _df_minority,\n",
        "    replace=True,\n",
        "    n_samples=len(_df_majority),\n",
        "    random_state=SEED,    # deterministic upsampling\n",
        ")\n",
        "\n",
        "_df_balanced = pd.concat([_df_majority, _df_minority_up], axis=0)\n",
        "_df_balanced = _df_balanced.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "X_train_proc = _df_balanced.drop(columns=['Churn']).values\n",
        "y_train = _df_balanced['Churn'].astype(int).values\n",
        "\n",
        "print(\"\\nAfter oversampling:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(pd.Series(y_train).value_counts(normalize=True))\n",
        "\n",
        "# ============================================\n",
        "# PyTorch Dataset / DataLoaders (deterministic)\n",
        "# ============================================\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # Handle pandas DataFrame / Series or numpy arrays\n",
        "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
        "            X = X.values\n",
        "        if isinstance(y, pd.Series):\n",
        "            y = y.values\n",
        "\n",
        "        self.X = torch.tensor(np.asarray(X), dtype=torch.float32)\n",
        "        self.y = torch.tensor(np.asarray(y), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "train_dataset = TabDataset(X_train_proc, y_train)\n",
        "val_dataset   = TabDataset(X_val_proc,   y_val)\n",
        "test_dataset  = TabDataset(X_test_proc,  y_test)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Deterministic data loader generator\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    generator=g,\n",
        "    num_workers=0,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "n_features = X_train_proc.shape[1]\n",
        "n_classes  = 2  # 0 = No, 1 = Yes\n",
        "\n",
        "# ============================================\n",
        "# TabTransformer-like model (features as tokens)\n",
        "# ============================================\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features: int,\n",
        "        d_model: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dim_feedforward: int = 128,\n",
        "        dropout: float = 0.1,\n",
        "        n_classes: int = 2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # One embedding vector per feature index\n",
        "        self.feature_emb = nn.Embedding(n_features, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, n_features) - already standardized / one-hot encoded\n",
        "        We treat each feature as a token. For feature i, representation is:\n",
        "        token_i = value_i * embedding(feature_index=i)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        feat_indices = torch.arange(self.n_features, device=x.device)\n",
        "        feat_emb = self.feature_emb(feat_indices)        # (n_features, d_model)\n",
        "        feat_emb = feat_emb.unsqueeze(0).expand(batch_size, -1, -1)  # (B, n_features, d_model)\n",
        "\n",
        "        x_expanded = x.unsqueeze(-1)                     # (B, n_features, 1)\n",
        "        tokens = feat_emb * x_expanded                   # (B, n_features, d_model)\n",
        "\n",
        "        h = self.transformer(tokens)                     # (B, n_features, d_model)\n",
        "        h = self.norm(h)\n",
        "\n",
        "        h_pool = h.mean(dim=1)                           # (B, d_model)\n",
        "        logits = self.cls_head(h_pool)                   # (B, n_classes)\n",
        "        return logits\n",
        "\n",
        "# ============================================\n",
        "# Training / evaluation helpers\n",
        "# ============================================\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_targets.append(y_batch.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    f1 = f1_score(all_targets, all_preds, pos_label=1, zero_division=0)\n",
        "    return avg_loss, f1\n",
        "\n",
        "\n",
        "def eval_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_targets = [], []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    f1 = f1_score(all_targets, all_preds, pos_label=1, zero_division=0)\n",
        "    return avg_loss, f1, all_targets, all_preds, all_probs\n",
        "\n",
        "# ============================================\n",
        "# Optuna objective for TabTransformer (deterministic)\n",
        "# ============================================\n",
        "PATIENCE = 5\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "# Replace F1-based early stopping with ROC AUC monitoring\n",
        "def objective_tabtransformer(trial):\n",
        "    d_model = trial.suggest_categorical(\"d_model\", [32, 64, 128])\n",
        "    n_heads = trial.suggest_categorical(\"n_heads\", [2, 4])\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    dim_feedforward = trial.suggest_categorical(\"dim_feedforward\", [64, 128, 256])\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    model = TabTransformer(\n",
        "        n_features=n_features,\n",
        "        d_model=d_model,\n",
        "        n_heads=n_heads,\n",
        "        n_layers=n_layers,\n",
        "        dim_feedforward=dim_feedforward,\n",
        "        dropout=dropout,\n",
        "        n_classes=n_classes,\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_auc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        train_loss, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_loss, val_f1, y_val_true, _, y_val_prob = eval_model(model, val_loader, criterion)\n",
        "        val_auc = roc_auc_score(y_val_true, y_val_prob)\n",
        "\n",
        "        trial.report(val_auc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        if val_auc > best_val_auc + 1e-5:\n",
        "            best_val_auc = val_auc\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"[Trial {trial.number}] early stopping at epoch {epoch+1} (best val AUC={best_val_auc:.4f})\")\n",
        "            break\n",
        "\n",
        "    return best_val_auc\n",
        "\n",
        "# Run Optuna study\n",
        "study_tab = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
        "    study_name=\"tabtransformer_fixed\",\n",
        ")\n",
        "study_tab.optimize(objective_tabtransformer, n_trials=100)\n",
        "\n",
        "print(\"Best trial:\", study_tab.best_trial.number)\n",
        "print(\"Best params:\", study_tab.best_params)\n",
        "print(\"Best val AUC:\", study_tab.best_value)\n",
        "\n",
        "# Train final model with best hyperparameters using val AUC early stopping\n",
        "best_params = study_tab.best_params\n",
        "best_model = TabTransformer(\n",
        "    n_features=n_features,\n",
        "    d_model=best_params[\"d_model\"],\n",
        "    n_heads=best_params[\"n_heads\"],\n",
        "    n_layers=best_params[\"n_layers\"],\n",
        "    dim_feedforward=best_params[\"dim_feedforward\"],\n",
        "    dropout=best_params[\"dropout\"],\n",
        "    n_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(best_model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "\n",
        "best_val_auc = 0.0\n",
        "epochs_no_improve = 0\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss, train_f1 = train_one_epoch(best_model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_f1, y_val_true, _, y_val_prob = eval_model(best_model, val_loader, criterion)\n",
        "    val_auc = roc_auc_score(y_val_true, y_val_prob)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "    if val_auc > best_val_auc + 1e-5:\n",
        "        best_val_auc = val_auc\n",
        "        best_state_dict = best_model.state_dict()\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"Early stopping triggered (best val AUC={best_val_auc:.4f}).\")\n",
        "        break\n",
        "\n",
        "best_model.load_state_dict(best_state_dict)\n",
        "\n",
        "# ============================================\n",
        "# Evaluate on test set\n",
        "# ============================================\n",
        "test_loss, test_f1, y_true, y_pred, y_proba = eval_model(best_model, test_loader, criterion)\n",
        "\n",
        "accuracy = (y_true == y_pred).mean()\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, pos_label=1, average=\"binary\", zero_division=0\n",
        ")\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "auc = roc_auc_score(y_true, y_proba)\n",
        "\n",
        "print(\"\\n=== TabTransformer Test Metrics ===\")\n",
        "print(f\"Accuracy   : {accuracy:.4f}\")\n",
        "print(f\"Precision  : {precision:.4f}\")\n",
        "print(f\"Recall     : {recall:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"F1 score   : {f1:.4f}\")\n",
        "print(f\"AUC        : {auc:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ============================================\n",
        "# Plot Confusion Matrix\n",
        "# ============================================\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"TabTransformer Confusion Matrix (Test)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Plot ROC Curve\n",
        "# ============================================\n",
        "fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"TabTransformer ROC Curve (Test)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
